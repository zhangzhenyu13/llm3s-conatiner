import openai
import mdtex2html
import gradio as gr
import requests
from utils import parse_text
from configs.supported import SupportedModels
supported_gen_modes=[
    "sample",
    "contrastive"
]
headers={
    'Content-Type': 'application/json'
}
model_id2name=SupportedModels.model_names
model_name2id={v:k for k, v in model_id2name.items() }


def streaming_generator(model_id, data):
    model_port = SupportedModels.model_port[model_id]
    model_host = SupportedModels.model_host[model_id]
    model_url = f"http://{model_host}:{model_port}"
    print("model request path", model_url)
    openai.api_base = f"http://{model_host}:{model_port}/v1"
    openai.api_key = "none"

    
    messages =[]
    chatbot, history = data['chatbot'], data['history']
    query = data['query']
    for (old_query, response) in history:
        messages.append({"role": "user", "content": old_query})
        messages.append({"role": "assistant", "content": response})
    messages.append({"role": "user", "content": query})
    
    chatbot.append((parse_text(query), ""))
    history.append((query, ""))
    response = ""
    for chunk in openai.ChatCompletion.create(
            model=model_id,
            messages=messages,
            decoder = data['decoder'],
            temperature=data['temperature'],
            top_p= data['top_p'],
            max_length= data['max_length'],
            repetition_penalty= data['repetition_penalty'],
            stream=True
        ):
            if hasattr(chunk.choices[0].delta, "content"):
                response += chunk.choices[0].delta.content
                chatbot[-1] = (query, parse_text(response))
                history[-1] = (query, response)
                yield chatbot, history

    # transform id to model-name
    model_name = model_id2name.get(model_id, model_id)
    if len(chatbot) > 0:
        last_qa = chatbot.pop()
        last_qa = (last_qa[0], last_qa[1] + "<br><br>" + "******"*10 + f"generated by {model_name}" +  "******"*10  )
        chatbot.append(last_qa)
        
    yield chatbot, history
    print("all over")

def request_streaming(model_id, user_input, chatbot, max_length, decoder_mode, top_p, temperature, repetition_penalty, history, do_reward):
    model_name = model_id
    # transform name to model id
    model_id = model_name2id.get(model_id, model_id)
    data = {
        "query": user_input, "chatbot": chatbot,
        "max_length": max_length, 
        "decoder": decoder_mode, 
        # "top_k": top_k, "penalty_alpha": penalty_alpha, 
        "top_p": float(top_p), "temperature": float(temperature), 
        "repetition_penalty": float(repetition_penalty),
        "history": history
    }
    if model_id not in SupportedModels.model_port:
        chatbot.append((user_input, f"model id not supported: {model_name}"))
        # history = history + [(user_input, f"model id not supported: {model_id}" ) ]
        yield chatbot, history
    elif not user_input.strip() :
        chatbot.append((user_input, f"empty user input"))
        # history = history + [(user_input, f"model id not supported: {model_id}" ) ]
        yield chatbot, history
    else:
        for chatbot, history in streaming_generator(model_id=model_id, data=data):
            yield chatbot, history

    if do_reward:
        rewardScore = None
        rewardScoreSigmoid = None
        reasonReward = ""

        reward_model_id = "your-org/rewardS2"
        model_port = SupportedModels.model_port[reward_model_id]
        model_host = SupportedModels.model_host[reward_model_id]
        model_url = f"http://{model_host}:{model_port}/ask"
        print("reward-model request path", model_url)
        x= {"max_length": max_length, "history": history}
        try:
            resReward = requests.post(url= model_url, json = x, headers=headers)
            resReward = resReward.json()
            # print("reward:", resReward)
            rewardScore = resReward['score']
            rewardScoreSigmoid = resReward.get('score+', None)
            reasonReward = resReward.get('reason', None)

        except :
            print("ERROR: reward-model",) # e.args)
            
        last_qa = chatbot.pop()
        last_qa = (last_qa[0], last_qa[1] + "<br>" + "******"*10 + \
            f"Reward==>context+response: {rewardScore}(sigmoid={rewardScoreSigmoid}, {reasonReward})" +  "******"*10  )
        chatbot.append(last_qa)
        yield chatbot, history

def request_regen(model_id, user_input, chatbot, max_length, decoder_mode, top_p, temperature, repetition_penalty, history, do_reward):
    if len(chatbot)>0:
        chatbot.pop()
    if len(history) >0:
        # print(history)
        # print(chatbot)
        x = history.pop()
        # chatbot.pop()
        user_input = x[0]
        print("regen for Q:", user_input)
    for chatbot, history in request_streaming(model_id, user_input, chatbot, max_length, decoder_mode, top_p, temperature, repetition_penalty, history, do_reward):
        yield chatbot, history

"""Override Chatbot.postprocess"""


def postprocess(self, y):
    if y is None:
        return []
    for i, (message, response) in enumerate(y):
        y[i] = (
            None if message is None else mdtex2html.convert((message)),
            None if response is None else mdtex2html.convert(response),
        )
    return y


gr.Chatbot.postprocess = postprocess


def reset_user_input():
    return gr.update(value='')


def reset_state():
    return [], []   

with gr.Blocks() as demo:
    gr.HTML("""<h1 align="center">大模型能力体验平台</h1>""")

    chatbot = gr.Chatbot()
    with gr.Row():
        with gr.Column(scale=4):
            with gr.Column(scale=12):
                user_input = gr.Textbox(show_label=False, placeholder="Input...", lines=10).style(
                    container=False)
            with gr.Row():
                with gr.Column(min_width=16, scale=1):
                    submitBtn = gr.Button("Submit", variant="primary")
                with gr.Column(min_width=16, scale=1):
                    regenBtn = gr.Button("Regenerate", variant="primary")
                with gr.Column(min_width=16, scale=1):
                    rewardSel = gr.Checkbox(value=False, interactive=True, label="Reward", info="get reward of the response." )
            # print("user input",user_input)
        with gr.Column(scale=2):
            emptyBtn = gr.Button("Clear History")
            max_length = gr.Slider(
                0, 4096, value=512, step=1.0, label="Maximum length", interactive=True)
            top_p = gr.Slider(0.01, 1.0, value=0.7, step=0.01,
                              label="Top P", interactive=True)
            temperature = gr.Slider(
                0.05, 1, value=0.95, step=0.01, label="Temperature", interactive=True)
            repetition_penalty = gr.Slider(
                1.0, 2.0, value=1.0, step=0.01, label="Repetition Penalty", interactive=True)
            
            # top_k = gr.Slider(1, 10, value=4, step=1,
            #                   label="Top K", interactive=True)
            # penalty_alpha = gr.Slider(
            #     0.05, 1, value=0.6, step=0.01, label="Penalty Alpha", interactive=True)
            decoder_mode = gr.components.Dropdown(choices=supported_gen_modes, 
                    label='Select Generation Decoding', type='value', value=supported_gen_modes[0])
            modelSet = [model_id2name.get(mod_id, mod_id) for mod_id in SupportedModels.model_ids if "reward" not in mod_id]
            model_id = gr.components.Dropdown(choices=modelSet, 
                    label='Select Model', type='value', value=modelSet[0])

                
    history = gr.State([])  # (message, bot_message)

    # gen_params = [decode_gen_mode, top_p, temperature, top_k, penalty_alpha ]
    submitBtn.click(request_streaming, [model_id, user_input, chatbot, max_length, decoder_mode, top_p, temperature, repetition_penalty, history, rewardSel], [chatbot, history],
                    show_progress=True)
    
    regenBtn.click(request_regen, [model_id, user_input, chatbot, max_length, decoder_mode, top_p, temperature, repetition_penalty, history, rewardSel], [chatbot, history],
                    show_progress=True)
    
    submitBtn.click(reset_user_input, [], [user_input])

    emptyBtn.click(reset_state, outputs=[chatbot, history], show_progress=True)

with open("configs/server.json") as f:
    import json
    server_config= json.load(f)

demo.queue(concurrency_count=server_config['gui_threads']).launch(share=False, inbrowser=False, debug=False,
    server_name= '0.0.0.0', server_port=server_config['gui_web'])
